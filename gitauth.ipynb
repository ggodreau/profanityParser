{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in api key\n",
    "credentials = json.load(open('./apikey.json', 'r'), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GHE docs:\n",
    "# curl -H \"Authorization: token OAUTH-TOKEN\" http(s)://[hostname]/api/v3  <-- endpoint\n",
    "''' \n",
    "Declare global variables\n",
    "'''\n",
    "baseurl = 'git.generalassemb.ly' # of course may be different for you\n",
    "header = {'Authorization': 'token {}'.format(credentials['token'])}\n",
    "source_org = 'Data-science-immersive' # change this as you see fit\n",
    "target_org = 'DSI-ME-1' # this one too\n",
    "repo_names = []\n",
    "data = { 'organization': target_org }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of all the orgs the API key has access to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsi-unit-3: id = 261\n",
      "DSI-ATX-3: id = 6931\n",
      "data-part-time: id = 6944\n",
      "DSI-EAST-1: id = 7793\n",
      "DSI-DC-6: id = 7895\n",
      "DSI-EAST-2: id = 9038\n",
      "Data-science-immersive: id = 9634\n",
      "DAT-ME-1: id = 11079\n",
      "DSI-ME-1: id = 11622\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Example calls below\n",
    "'''\n",
    "# call the base endpoint\n",
    "'''\n",
    "r = requests.get('https://{}/api/v3'.format(baseurl), headers=header)\n",
    "'''\n",
    "\n",
    "# get a listing of orgs the user's api key has access to\n",
    "\n",
    "r = requests.get('https://{}/api/v3/user/orgs'.format(baseurl), headers=header)\n",
    "orgs = json.loads(r.content)\n",
    "for org in orgs:\n",
    "    print('{}: id = {}'.format(org['login'], org['id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a dataframe with all the repos from that org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_repos(baseurl, source_org, startpage=1):\n",
    "    # set initial page to fetch\n",
    "    page = startpage\n",
    "    # create blank dataframe to put results in\n",
    "    df_dict = {\n",
    "        'id': [], \n",
    "        'name': [],\n",
    "        'full_name': [],\n",
    "        'ssh_url': [],\n",
    "        'created_at': [],\n",
    "        'updated_at': [],\n",
    "        'pushed_at': [],\n",
    "        'page': [] # this doesn't come from the api\n",
    "    }\n",
    "    repo_df = pd.DataFrame.from_dict(df_dict).set_index('id')\n",
    "    # begin fetchin those shits\n",
    "    while True:\n",
    "        r = requests.get('https://{}/api/v3/orgs/{}/repos'.format(baseurl, source_org), \\\n",
    "                 headers=header, params={ 'type': 'all' , 'page': page })\n",
    "        try:\n",
    "            # see if there's data in the response\n",
    "            repos = r.json()\n",
    "            repos[0]\n",
    "            print('Successfully fetched page {}'.format(page))\n",
    "\n",
    "            for repo in repos:\n",
    "                for k in df_dict.keys():\n",
    "                    try:\n",
    "                        df_dict[k].append(repo[k])\n",
    "                    # this is kinda a hack to insert the page num we're on\n",
    "                    except KeyError:\n",
    "                        df_dict['page'].append(page)\n",
    "            # put the values from the dict into res_df\n",
    "            res_df = pd.DataFrame.from_dict(df_dict).set_index('id')\n",
    "            # clear dict for next page from api\n",
    "            for k in df_dict.keys():\n",
    "                df_dict[k] = []\n",
    "            # append res_df to initial blank dataframe\n",
    "            repo_df = repo_df.copy().append(res_df)\n",
    "\n",
    "            page += 1\n",
    "        except IndexError:\n",
    "            print('Page {} returned no data, exiting...'.format(page))\n",
    "            \n",
    "            # change the index from float to int\n",
    "            repo_df.set_index(repo_df.index.astype(int), inplace=True)\n",
    "\n",
    "            # make the date/time cols into ....datetime\n",
    "            for col in ['created_at', 'updated_at', 'pushed_at']:\n",
    "                repo_df[col] = pd.to_datetime(repo_df.copy()[col])\n",
    "\n",
    "            # change page numbers from floats to ints\n",
    "            repo_df['page'] = repo_df.copy()['page'].apply(lambda x: int(x))   \n",
    "            \n",
    "            return repo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched page 1\n",
      "Successfully fetched page 2\n",
      "Successfully fetched page 3\n",
      "Successfully fetched page 4\n",
      "Successfully fetched page 5\n",
      "Page 6 returned no data, exiting...\n"
     ]
    }
   ],
   "source": [
    "data_science_immersive_df = get_all_repos(baseurl, source_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_science_immersive_df.to_csv('./data-science-immersive.csv', index_label='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code below will clone an entire org's repos to a local folder; use with parseme.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NEEDS CODE WRITTEN TO TRAVERSE PAGES (start at page 1, go until r.status_code = 202)\n",
    "see docs here:\n",
    "https://developer.github.com/v3/repos/#parameters-2\n",
    "\n",
    "Note: the 'page' parameter is not documented with the github API, I found it by trial/error\n",
    "'''\n",
    "\n",
    "# get a listing of all repos in the target org, 'Data-science-immersive'\n",
    "# from GHE docs: GET /orgs/:org/repos\n",
    "\n",
    "r = requests.get('https://{}/api/v3/orgs/{}/repos'.format(baseurl, source_org), \\\n",
    "                 headers=header, params={ 'type': 'all' , 'page': 1 })\n",
    "repos = json.loads(r.content)\n",
    "\n",
    "clones = []\n",
    "for repo in repos:\n",
    "    clones.append(repo['ssh_url'])\n",
    "    \n",
    "# show first 5\n",
    "for repo in repos[:5]:\n",
    "    print('{}, {}'.format(repo['name'], repo['ssh_url']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This will create a place for all the cloned repos to live.\n",
    "Use the parseme file, pointing to this directory, to\n",
    "scan all clones repos for words\n",
    "'''\n",
    "\n",
    "%%bash\n",
    "if [ ! -d \"./course_dump\" ]; then\n",
    "    mkdir ./course_dump\n",
    "    echo \"created directory ./course_dump\"\n",
    "else\n",
    "    echo \"directory already exists: ./course_dump\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING!!! ===\n",
    "# this will clone ALL repositories in the organization to ./course_dump!!\n",
    "# note - has bug; will not retry cloning of repository if network fails :'(\n",
    "\n",
    "try:\n",
    "    for repo in clones:\n",
    "        # made it sleepy to be a good citizen\n",
    "        !cd ./course_dump && git clone $repo && sleep 1\n",
    "    print(\"Finished cloning {} repositories into ./course_dump\".format(len(clones)))\n",
    "except:\n",
    "    # outstanding error handling here\n",
    "    print(\"noooooo!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will fork all repositories from a source org to a target org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://{}/api/v3/orgs/{}/repos'.format(baseurl, source_org), \\\n",
    "                 headers=header)\n",
    "repos = json.loads(r.content)\n",
    "\n",
    "fork_urls = []\n",
    "for repo in repos:\n",
    "    fork_urls.append(repo['forks_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the target org parameter, where the forks will go\n",
    "data = { 'organization': target_org }\n",
    "\n",
    "'''\n",
    "This needs to be done as a session because the GET request does the\n",
    "auth, and the POST request does the forking. You CAN'T pass a cookie\n",
    "between the two sessions, I tried. Couldn't find any examples of anybody\n",
    "doing this on SO, etc. Was able to get it working with CURL but wanted\n",
    "to use python requests lib. Trick is passing the header with BOTH the \n",
    "GET *AND* the POST requests. You can't just send the header with the \n",
    "POST request, despite what the docs lead you to believe.\n",
    "\n",
    "School of hard knocks... damn it feels good to be a gangster\n",
    "'''\n",
    "\n",
    "with requests.Session() as s:\n",
    "    # this get request authenticates the user\n",
    "    r1 = s.get('https://{}/api/v3'.format(baseurl), headers=header)\n",
    "    # ...and then this post request creates the fork\n",
    "    for fork_url in fork_urls:\n",
    "        r2 = s.post(fork_url, headers=header, data=json.dumps(data))\n",
    "        print('{} successfully forked to {}'.format( \\\n",
    "            fork_url.split('/')[-3] + '/' + fork_url.split('/')[-2], \\\n",
    "            json.loads(r2.text)['full_name'])\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
